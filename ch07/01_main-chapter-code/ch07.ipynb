{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e91914-5f51-43fa-b65b-625e73b4d17b",
   "metadata": {
    "id": "12e91914-5f51-43fa-b65b-625e73b4d17b"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp?1\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf",
   "metadata": {
    "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf"
   },
   "source": [
    "# Chapter 7: Finetuning To Follow Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
    "outputId": "bcdfe2cb-d084-4920-d703-503131aabec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.0.2\n",
      "matplotlib version: 3.10.3\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.7.0\n",
      "tqdm version: 4.67.1\n",
      "tensorflow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"numpy\",       # PyTorch & TensorFlow dependency\n",
    "    \"matplotlib\",  # Plotting library\n",
    "    \"tiktoken\",    # Tokenizer\n",
    "    \"torch\",       # Deep learning library\n",
    "    \"tqdm\",        # Progress bar\n",
    "    \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264fca98-2f9a-4193-b435-2abfa3b4142f",
   "metadata": {
    "id": "264fca98-2f9a-4193-b435-2abfa3b4142f"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/overview.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc68e9-75b3-41f1-ac2c-e071c3cd0813",
   "metadata": {
    "id": "8bbc68e9-75b3-41f1-ac2c-e071c3cd0813"
   },
   "source": [
    "## 7.1 Introduction to instruction finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dba24a-6805-496c-9a7f-c75e2d3527ab",
   "metadata": {
    "id": "53dba24a-6805-496c-9a7f-c75e2d3527ab"
   },
   "source": [
    "- In chapter 5, we saw that pretraining an LLM involves a training procedure where it learns to generate one word at a time\n",
    "- Hence, a pretrained LLM is good at text completion, but it is not good at following instructions\n",
    "- In this chapter, we teach the LLM to follow instructions better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc0535-0904-44ed-beaf-9b678292ef35",
   "metadata": {
    "id": "18dc0535-0904-44ed-beaf-9b678292ef35"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/instruction-following.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4698b23-12e0-4bd7-a140-ccb3dd71d4e8",
   "metadata": {
    "id": "b4698b23-12e0-4bd7-a140-ccb3dd71d4e8"
   },
   "source": [
    "- The topics covered in this chapter are summarized in the figure below\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-1.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384f0cf-ef3c-4436-a5fa-59bd25649f86",
   "metadata": {
    "id": "5384f0cf-ef3c-4436-a5fa-59bd25649f86"
   },
   "source": [
    "## 7.2 Preparing a dataset for supervised instruction finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b34ff8-619f-4e89-bd03-ce513269760d",
   "metadata": {
    "id": "f8b34ff8-619f-4e89-bd03-ce513269760d"
   },
   "source": [
    "- We will work with an instruction dataset I prepared for this chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda92c1b",
   "metadata": {},
   "source": [
    "代码实现并执行了一个函数来下载这个数据集，该数据集保存在一个相对较小的 JSON 格式的文件中（仅 204 KB）​。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0G3axLw6kY1N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0G3axLw6kY1N",
    "outputId": "07e1e4f9-026c-48c1-8a06-f2bfb1fb354e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    # The book originally contained this unnecessary \"else\" clause:\n",
    "    #else:\n",
    "    #    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    #        text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af8176-4255-4e92-8c7d-998771733eb8",
   "metadata": {
    "id": "d7af8176-4255-4e92-8c7d-998771733eb8"
   },
   "source": [
    "- Each item in the `data` list we loaded from the JSON file above is a dictionary in the following form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "-LiuBMsHkzQV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LiuBMsHkzQV",
    "outputId": "a4ee5c2d-db53-4a80-e5ee-0bbcf6fe0450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a32b34-485a-4816-a77a-da14f9fe6e46",
   "metadata": {
    "id": "c5a32b34-485a-4816-a77a-da14f9fe6e46"
   },
   "source": [
    "- Note that the `'input'` field can be empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "uFInFxDDk2Je",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFInFxDDk2Je",
    "outputId": "b4f84027-bb9e-4e51-b79e-1329c8bff093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034799a-6575-45fd-98c9-9d1012d0fd58",
   "metadata": {
    "id": "f034799a-6575-45fd-98c9-9d1012d0fd58"
   },
   "source": [
    "- Instruction finetuning is often referred to as \"supervised instruction finetuning\" because it involves training a model on a dataset where the input-output pairs are explicitly provided\n",
    "- There are different ways to format the entries as inputs to the LLM; the figure below illustrates two example formats that were used for training the Alpaca (https://crfm.stanford.edu/2023/03/13/alpaca.html) and Phi-3 (https://arxiv.org/abs/2404.14219) LLMs, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0254b6",
   "metadata": {},
   "source": [
    "展示了两种样本格式，这通常也被称为提示词风格，常用于训练知名的大语言模型，比如 Alpaca 和 Phi-3。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa4f70-44d4-4be4-89a9-2159f4885b10",
   "metadata": {
    "id": "dffa4f70-44d4-4be4-89a9-2159f4885b10"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/prompt-style.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a34029",
   "metadata": {},
   "source": [
    "大语言模型指令微调中不同提示词风格的比较。Alpaca 风格（左）为指令、输入和回复定义了不同的小节，其采用的是结构化的形式；Phi-3 风格（右）则使用了更简单的形式，主要借助的是特殊词元 <|user|> 和 <|assistant|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd79a74e-befb-491c-be49-f777a6a5b6a6",
   "metadata": {
    "id": "dd79a74e-befb-491c-be49-f777a6a5b6a6"
   },
   "source": [
    "- In this chapter, we use Alpaca-style prompt formatting, which was the original prompt template for instruction finetuning\n",
    "- Below, we format the input that we will pass as input to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e277efe",
   "metadata": {},
   "source": [
    "下面定义一个 format_input 函数，然后我们可以使用它将 data 列表中的样本转换成Alpaca 风格的输入格式，如代码清单 7-2 所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Jhk37nnJnkBh",
   "metadata": {
    "id": "Jhk37nnJnkBh"
   },
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    \"\"\"格式化Alpaca风格的指令输入\n",
    "    \n",
    "    Args:\n",
    "        entry (dict): 包含指令和输入的字典，包含两个键：\n",
    "            instruction (str): 任务指令文本\n",
    "            input (str): 可选的输入上下文\n",
    "            \n",
    "    Returns:\n",
    "        str: 格式化后的完整提示文本，包含指令和输入（如果有）\n",
    "    \"\"\"\n",
    "    # 构建指令基础模板\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    # 添加可选输入部分（当输入非空时）\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e78b4-e89a-4653-a2ee-7b2739ca04d6",
   "metadata": {
    "id": "011e78b4-e89a-4653-a2ee-7b2739ca04d6"
   },
   "source": [
    "- A formatted response with input field looks like as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "F9UQRfjzo4Js",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9UQRfjzo4Js",
    "outputId": "7b615d35-2a5f-474d-9292-a69bc3850e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc93ddf-431c-49c0-96f2-fb3a79c4d94c",
   "metadata": {
    "id": "4dc93ddf-431c-49c0-96f2-fb3a79c4d94c"
   },
   "source": [
    "- Below is a formatted response without an input field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bc831",
   "metadata": {},
   "source": [
    "值得注意的是，如果 'input' 键对应的值是空的，那么 format_input 函数就会跳过可选的 ### Input: 部分。可以把 format_input 函数用在我们之前检查过的 data[999]上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3891fa9-f738-41cd-946c-80ef9a99c346",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3891fa9-f738-41cd-946c-80ef9a99c346",
    "outputId": "2142c5a4-b594-49c5-affe-2d963a7bd46b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8afd5-2a21-49a5-90c3-6a03865a4771",
   "metadata": {
    "id": "4aa8afd5-2a21-49a5-90c3-6a03865a4771"
   },
   "source": [
    "- Lastly, before we prepare the PyTorch data loaders in the next section, we divide the dataset into a training, validation, and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e8d9f",
   "metadata": {},
   "source": [
    "在设置 PyTorch 数据集加载器之前，还需要将数据集分为训练集、验证集和测试集，所用方法与我们在第6章中处理垃圾消息分类数据集时相似。代码清单 7-3 展示了如何设置这些数据集的比例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aFZVopbIlNfx",
   "metadata": {
    "id": "aFZVopbIlNfx"
   },
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "-zf6oht6bIUQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zf6oht6bIUQ",
    "outputId": "657ec5c6-4caa-4d1a-ba2e-23acd755ab07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaf606-f913-4445-8301-632ae10d387d",
   "metadata": {
    "id": "fcaaf606-f913-4445-8301-632ae10d387d"
   },
   "source": [
    "## 7.3 Organizing data into training batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f63bd-9755-4d07-8884-5e2e5345cf27",
   "metadata": {
    "id": "233f63bd-9755-4d07-8884-5e2e5345cf27"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-2.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149fc1a-7757-4ec8-80cb-e2a3fb007a2c",
   "metadata": {
    "id": "c149fc1a-7757-4ec8-80cb-e2a3fb007a2c"
   },
   "source": [
    "- We tackle this dataset batching in several steps, as summarized in the figure below\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/detailed-batching.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d110cca",
   "metadata": {},
   "source": [
    "实现批处理过程包括以下 5 个子步骤：\n",
    "- (2.1) 应用提示词模板；\n",
    "- (2.2) 使用前几章提到的词元化方法；\n",
    "- (2.3) 添加填充词元；\n",
    "- (2.4) 创建目标词元 ID；\n",
    "- (2.5) 在损失函数中用 -100 占位符词元来掩码填充词元"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af423f-aad9-4b3c-bea5-153021c04862",
   "metadata": {
    "id": "b9af423f-aad9-4b3c-bea5-153021c04862"
   },
   "source": [
    "- First, we implement an `InstructionDataset` class that pre-tokenizes all inputs in the dataset, similar to the `SpamDataset` in chapter 6\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/pretokenizing.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e0f56",
   "metadata": {},
   "source": [
    "实现一个指令数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb",
   "metadata": {
    "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        \"\"\"数据集初始化方法\n",
    "        \n",
    "        Args:\n",
    "            data: 原始指令数据集，每个元素应包含instruction/input/output字段\n",
    "            tokenizer: 用于文本编码的分词器\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            # 生成指令+输入部分（使用之前定义的format_input函数）\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            # 构建响应部分的模板\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            # 组合完整文本：指令 + 输入 + 响应\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)  # 对完整文本进行编码\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f0e69-4b22-41c0-a25d-f077527eddd1",
   "metadata": {
    "id": "384f0e69-4b22-41c0-a25d-f077527eddd1"
   },
   "source": [
    "- Similar to chapter 6, we want to collect multiple training examples in a batch to accelerate training; this requires padding all inputs to a similar length\n",
    "- Also similar to the previous chapter, we use the `<|endoftext|>` token as a padding token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608f4d3",
   "metadata": {},
   "source": [
    "一个值得注意的细节是，可以直接将 <|endoftext|> 对应的词元 ID 拼接到预词元化的模型输入中，而无须将 <|endoftext|> 拼接在输入文本的末尾。可以使用分词器的.encode 方法对 <|endoftext|> 进行编码，以确定应该使用哪个词元 ID："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
    "outputId": "ac44227b-9ec2-4131-9df8-89caa6e879ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# 导入OpenAI的分词库（也适用于GPT-2）\n",
    "import tiktoken\n",
    "\n",
    "# 初始化GPT-2分词器（与GPT-3使用的分词方式兼容）\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# 对结束标记进行编码测试，显式允许处理特殊符号<|endoftext|>\n",
    "# allowed_special参数用于控制哪些特殊符号应该被保留\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bd7bc-f347-4cf8-a0c2-94cb8799e427",
   "metadata": {
    "id": "9e5bd7bc-f347-4cf8-a0c2-94cb8799e427"
   },
   "source": [
    "- In chapter 6, we padded all examples in a dataset to the same length\n",
    "  - Here, we take a more sophisticated approach and develop a custom \"collate\" function that we can pass to the data loader\n",
    "  - This custom collate function pads the training examples in each batch to have the same length (but different batches can have different lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4d943-4aa8-4a44-874e-05bc6831fbd3",
   "metadata": {
    "id": "65c4d943-4aa8-4a44-874e-05bc6831fbd3"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/padding.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7604f8c",
   "metadata": {},
   "source": [
    "使用词元 ID 50256 对批次中的训练样本进行填充，以确保每个批次的长度一致。但每个批次的长度可能不同，比如第一批数据就与第二批数据长度不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca",
   "metadata": {
    "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca"
   },
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"自定义批处理整理函数（初版）\n",
    "    \n",
    "    Args:\n",
    "        batch: 输入批次数据，每个元素是token id列表\n",
    "        pad_token_id: 填充token的ID（默认GPT-2的结束符ID）\n",
    "        device: 输出张量的目标设备\n",
    "        \n",
    "    Returns:\n",
    "        inputs_tensor: 填充对齐后的输入张量\n",
    "    \"\"\"\n",
    "    # 计算批次最大长度时+1，为后续处理留出空间（如目标序列生成）\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # 初始化输入列表\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()  # 避免修改原始数据\n",
    "        # 添加结束标记作为序列终止符\n",
    "        new_item += [pad_token_id]\n",
    "        # 填充到批次最大长度（包含额外的一个填充位置）\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # 截断最后一个填充位，保留给后续处理使用\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # 堆叠张量并转移设备（如GPU）\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
    "outputId": "93d987b9-e3ca-4857-9b28-b67d515a94d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46832ab-39b7-45f8-b330-ac9adfa10d1b",
   "metadata": {
    "id": "c46832ab-39b7-45f8-b330-ac9adfa10d1b"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/batching-step-4.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733cf0e",
   "metadata": {},
   "source": [
    "我们刚刚实现了第一个自定义的聚合函数，用于从输入列表中创建批次。然而，正如之前所提到的，我们还需要生成与输入词元 ID 批次对应的目标词元 ID。这些目标词元 ID（参见图7-9）非常重要，因为它们代表我们期望模型生成的内容，并且在训练中用来计算损失，以便进行权重更新。因此，我们需要对自定义聚合函数进行修改，以便除了输入词元 ID 之外，还能返回目标词元 ID。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17769a19-b961-4213-92ef-34f441b2d1d6",
   "metadata": {
    "id": "17769a19-b961-4213-92ef-34f441b2d1d6"
   },
   "source": [
    "- Above, we only returned the inputs to the LLM; however, for LLM training, we also need the target values\n",
    "- Similar to pretraining an LLM, the targets are the inputs shifted by 1 position to the right, so the LLM learns to predict the next token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386b6fe-3455-4e70-becd-a5a4681ba2ef",
   "metadata": {
    "id": "0386b6fe-3455-4e70-becd-a5a4681ba2ef"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/inputs-targets.webp?1\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66969988",
   "metadata": {},
   "source": [
    "大语言模型指令微调过程中使用的输入词元和目标词元之间的对应关系。对每个输入序列而言，首先将其向左移动一个词元的位置，然后将输入序列的第一个词元忽略，最后在尾部加入结束符词元即可得到其对应的目标序列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d82f3c",
   "metadata": {},
   "source": [
    "实现批处理过程包括 5 个子步骤。此刻我们关注第(2.4) 步，这一步构建了目标词元 ID。这一步至关重要，因为它使得模型能够学习并预测需要生成的词元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc",
   "metadata": {
    "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc"
   },
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"自定义批处理整理函数（第二版），生成输入-目标对\n",
    "    \n",
    "    Args:\n",
    "        batch: 输入批次数据，每个元素是token id列表\n",
    "        pad_token_id: 填充token的ID（默认GPT-2的结束符ID）\n",
    "        device: 输出张量的目标设备\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (输入张量, 目标张量) 目标张量是输入右移一位的结果\n",
    "    \"\"\"\n",
    "    # 计算批次最大长度时+1，为右移目标留出空间\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # 初始化输入和目标列表\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()  # 防止修改原始数据\n",
    "        # 添加结束标记作为序列终止符\n",
    "        new_item += [pad_token_id]\n",
    "        # 填充到批次最大长度（包含右移需要的额外位置）\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # 输入取前n-1个token，目标取后n-1个token（实现右移）\n",
    "        inputs = torch.tensor(padded[:-1])  # 输入序列\n",
    "        targets = torch.tensor(padded[1:])  # 右移后的目标序列\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # 堆叠张量并转移设备（如GPU），保持输入与目标对齐\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
    "outputId": "3d104439-c328-431b-ef7c-2639d86c2135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d7904",
   "metadata": {},
   "source": [
    "在下一步中，我们会为所有填充词元都分配一个 -100 占位符值（参见图7-11 突出显示的部分）​。这个特殊值使我们能够在计算训练损失时排除填充词元的影响，从而确保只有有效的数据会影响模型的学习。我们将在实现此修改后更详细地讨论这一过程。​（值得说明的是，分类微调时无须担心这个问题，因为我们只根据最后的输出词元对模型进行训练。​）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf85703-a0e0-42aa-8f29-cbc28dbf4e15",
   "metadata": {
    "id": "3bf85703-a0e0-42aa-8f29-cbc28dbf4e15"
   },
   "source": [
    "- Next, we introduce an `ignore_index` value to replace all padding token IDs with a new value; the purpose of this `ignore_index` is that we can ignore padding values in the loss function (more on that later)\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/batching-step-5.webp?1\" width=500px>\n",
    "\n",
    "- Concretely, this means that we replace the token IDs corresponding to `50256` with `-100` as illustrated below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166f4de",
   "metadata": {},
   "source": [
    "实现批处理过程包括 5 个子步骤。在创建目标序列的过程中，我们将输入词元序列向左移动一个位置并附加一个结束符词元。接下来，在第(2.5) 步中，我们将结束符（填充）词元替换为占位符值(-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c165e57",
   "metadata": {},
   "source": [
    "不过，值得注意的是，我们在目标列表中保留了一个结束符词元，ID 为 50256，如图7-12 所示。保留此词元有助于大语言模型学会何时根据指令生成结束符词元，一般我们将其作为生成的回复已经完成的指示符。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4bed33-956e-4b3f-a09c-586d8203109a",
   "metadata": {
    "id": "bd4bed33-956e-4b3f-a09c-586d8203109a"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/ignore-index.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346513e-c3f4-44fe-af22-4ebd36497728",
   "metadata": {
    "id": "5346513e-c3f4-44fe-af22-4ebd36497728"
   },
   "source": [
    "- (In addition, we also introduce the `allowed_max_length` in case we want to limit the length of the samples; this will be useful if you plan to work with your own datasets that are longer than the 1024 token context size supported by the GPT-2 model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5c8d6",
   "metadata": {},
   "source": [
    "我们修改了自定义聚合函数，以将目标列表中 ID 为 50256的词元替换为 -100。此外，我们还引入了一个 allowed_max_length 参数，以选择性地限制样本的长度。这一调整在处理超过GPT-2 模型支持的 1024 个词元上下文大小的数据集时将非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2",
   "metadata": {
    "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2"
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"自定义批处理整理函数（最终版）\n",
    "    \n",
    "    Args:\n",
    "        batch: 输入批次数据，每个元素是token id列表\n",
    "        pad_token_id: 填充token的ID（默认GPT-2的结束符ID）\n",
    "        ignore_index: 需要忽略的损失计算位置（默认-100）\n",
    "        allowed_max_length: 允许的最大序列长度（None表示不限制）\n",
    "        device: 输出张量的目标设备\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (输入张量, 目标张量) 目标张量中后续填充位置被标记为忽略\n",
    "    \"\"\"\n",
    "    # 计算批次最大长度时+1，为右移目标留出空间\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # 初始化输入和目标列表\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        # ... [existing copy and padding logic] ...\n",
    "\n",
    "        # 创建掩码标记后续填充位置（仅保留第一个填充符用于损失计算）\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index  # 后续填充位置设为忽略值\n",
    "\n",
    "        # 可选长度截断（防止超出模型最大长度限制）\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]  # 截断输入序列\n",
    "            targets = targets[:allowed_max_length]  # 同步截断目标序列\n",
    "\n",
    "        # ... [existing appends] ...\n",
    "\n",
    "    # 堆叠张量并转移设备，保持输入与目标对齐\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"自定义批处理整理函数（最终版）\n",
    "    \n",
    "    Args:\n",
    "        batch: 输入批次数据，每个元素是token id列表\n",
    "        pad_token_id: 填充token的ID（默认GPT-2的结束符ID）\n",
    "        ignore_index: 需要忽略的损失计算位置（默认-100）\n",
    "        allowed_max_length: 允许的最大序列长度（None表示不限制）\n",
    "        device: 输出张量的目标设备\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (输入张量, 目标张量) 目标张量中后续填充位置被标记为忽略\n",
    "    \"\"\"\n",
    "    # 计算批次最大长度时+1，为右移目标留出空间\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # 初始化输入和目标列表\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()  # 防止修改原始数据\n",
    "        # 添加结束标记作为序列终止符\n",
    "        new_item += [pad_token_id]\n",
    "        # 填充到批次最大长度（包含右移需要的额外位置）\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # 输入取前n-1个token，目标取后n-1个token（实现右移）\n",
    "        inputs = torch.tensor(padded[:-1])  # 输入序列\n",
    "        targets = torch.tensor(padded[1:])  # 右移后的目标序列\n",
    "\n",
    "        # 创建掩码标记后续填充位置（仅保留第一个填充符用于损失计算）\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index  # 后续填充位置设为忽略值\n",
    "\n",
    "        # 可选长度截断（防止超出模型最大长度限制）\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]  # 截断输入序列\n",
    "            targets = targets[:allowed_max_length]  # 同步截断目标序列\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # 堆叠张量并转移设备，保持输入与目标对齐\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
    "outputId": "e8f709b9-f4c5-428a-a6ac-2a4c1b9358ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3f316",
   "metadata": {},
   "source": [
    "看起来修改后的聚合函数在正常工作，它成功地在目标列表对应位置插入了词元 ID -100。但是，这一调整背后的逻辑是什么呢？让我们探讨一下此修改的根本目的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26727c90-0d42-43b3-af21-0a66ad4fbbc7",
   "metadata": {
    "id": "26727c90-0d42-43b3-af21-0a66ad4fbbc7"
   },
   "source": [
    "- Let's see what this replacement by -100 accomplishes\n",
    "- For illustration purposes, let's assume we have a small classification task with 2 class labels, 0 and 1, similar to chapter 6\n",
    "- If we have the following logits values (outputs of the last layer of the model), we calculate the following loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6d654",
   "metadata": {},
   "source": [
    "为方便理解，可以考虑一个简单的示例，其中输出逻辑值(logits)的每一维都对应着模型词汇表中的一个潜在词元。下面的代码展示了在训练过程中交叉熵损失（参见第5章）是如何计算的，这一过程与我们在预训练和分类微调模型时的操作类似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "W2jvh-OP9MFV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2jvh-OP9MFV",
    "outputId": "ccb3a703-59a7-4258-8841-57959a016e31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],  # 1st training example\n",
    "     [-0.5, 1.5]]  # 2nd training example\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd3244-8886-4505-92e9-367d28529e1e",
   "metadata": {
    "id": "5edd3244-8886-4505-92e9-367d28529e1e"
   },
   "source": [
    "- Now, adding one more training example will, as expected, influence the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b22cc0",
   "metadata": {},
   "source": [
    "正如预期的那样，增加一个额外的词元会影响损失的计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "nvVMuil89v9N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvVMuil89v9N",
    "outputId": "6d4683d4-5bfc-4a8c-de2a-95ecb2e716b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]  # New 3rd training example\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dca331-40e0-468b-b690-189fe156ba8f",
   "metadata": {
    "id": "54dca331-40e0-468b-b690-189fe156ba8f"
   },
   "source": [
    "- Let's see what happens if we replace the class label of one of the examples with -100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7a069",
   "metadata": {},
   "source": [
    "到目前为止，我们已经使用 PyTorch的交叉熵损失函数进行了若干简单示例计算，这个损失函数正是我们在预训练和分类微调时使用的损失函数。接下来，来看一个有趣的情况：如果将第三个目标词元 ID 替换为 -100，会发生什么呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "RTyB1vah9p56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTyB1vah9p56",
    "outputId": "da05302e-3fe0-439e-d1ed-82066bceb122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef09d21-b652-4760-abea-4f76920e6a25",
   "metadata": {
    "id": "cef09d21-b652-4760-abea-4f76920e6a25"
   },
   "source": [
    "- As we can see, the resulting loss on these 3 training examples is the same as the loss we calculated from the 2 training examples, which means that the cross-entropy loss function ignored the training example with the -100 label\n",
    "- By default, PyTorch has the `cross_entropy(..., ignore_index=-100)` setting to ignore examples corresponding to the label -100\n",
    "- Using this -100 `ignore_index`, we can ignore the additional end-of-text (padding) tokens in the batches that we used to pad the training examples to equal length\n",
    "- However, we don't want to ignore the first instance of the end-of-text (padding) token (50256) because it can help signal to the LLM when the response is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e9c5f-7c49-4321-9f1b-a50468a84524",
   "metadata": {
    "id": "6a4e9c5f-7c49-4321-9f1b-a50468a84524"
   },
   "source": [
    "- In practice, it is also common to mask out the target token IDs that correspond to the instruction, as illustrated in the figure below (this is a recommended reader exercise after completing the chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef25396",
   "metadata": {},
   "source": [
    "- 得到的损失与之前示例计算中的损失相同。换言之，此时交叉熵损失函数忽略了targets_3 向量中的第三项(-100)所对应的损失。​（如果你对此感兴趣，可以尝试将-100 替换为其他非 0 或 1的词元，你将会发现错误。​）\n",
    "- 那么，-100 究竟有什么特别之处，使交叉熵损失能够忽略它呢？原来，在 PyTorch中，交叉熵函数的默认设置为 cross_entropy(..., ignore_index=-100)。这意味着它会忽略标记为 -100的目标。我们利用这个 ignore_index 来忽略那些用于填充训练示例以使每个批次具有相同长度的额外结束符（填充）词元。然而，我们需要在目标中保留结束符词元 ID 50256，因为它有助于大语言模型学习生成结束符词元，从而在适当的时候结束回复。\n",
    "- 除了掩码填充词元，实践中我们通常还会掩码与指令相关的目标词元，如图7-13 所示。通过掩码与指令对应的目标词元，交叉熵损失可以仅针对生成的回复目标词元进行计算。因此，模型的训练更专注于生成准确的回复，而非记住指令，这样可以帮助减少过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39",
   "metadata": {
    "id": "fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/mask-instructions.webp?1\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a50eca",
   "metadata": {},
   "source": [
    "训练期间，格式化输入文本被词元化并送入大语言模型中（左）​；大语言模型准备的目标文本，我们可以选择掩码指令部分，即将相应的词元替换为损失的ignore_index 值 -100（右）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96",
   "metadata": {
    "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96"
   },
   "source": [
    "## 7.4 Creating data loaders for an instruction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8e656-3af3-4db6-8dde-d8c216a12f50",
   "metadata": {
    "id": "e6b8e656-3af3-4db6-8dde-d8c216a12f50"
   },
   "source": [
    "- In this section, we use the `InstructionDataset` class and `custom_collate_fn` function to instantiate the training, validation, and test data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fffe390-b226-4d5c-983f-9f4da773cb82",
   "metadata": {
    "id": "9fffe390-b226-4d5c-983f-9f4da773cb82"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-3.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932677e9-9317-42e8-b461-7b0269518f97",
   "metadata": {
    "id": "932677e9-9317-42e8-b461-7b0269518f97"
   },
   "source": [
    "- Another additional detail of the previous `custom_collate_fn` function is that we now directly move the data to the target device (e.g., GPU) instead of doing it in the main training loop, which improves efficiency because it can be carried out as a background process when we use the `custom_collate_fn` as part of the data loader\n",
    "- Using the `partial` function from Python's `functools` standard library, we create a new function with the `device` argument of the original function pre-filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "etpqqWh8phKc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etpqqWh8phKc",
    "outputId": "b4391c33-1a89-455b-faaa-5f874b6eb409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44527bc8",
   "metadata": {},
   "source": [
    "为了在将 custom_collate_fn 函数应用于 PyTorch DataLoader 类时重用所选择的设备设置，我们利用 Python的 functools 标准库中的 partial 函数创建该函数的新版本并预先填充设备参数。此外，可以将 allowed_max_length 设置为 1024，这样数据就会被截断到 GPT-2 模型支持的最大上下文长度，稍后我们将对其进行微调。\n",
    "\n",
    "这种写法常用于：\n",
    "\n",
    "- 简化重复的函数调用\n",
    "- 保持参数配置的一致性\n",
    "- 适配需要固定参数的接口（如PyTorch DataLoader的collate_fn）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c",
   "metadata": {
    "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# 创建定制化的数据整理函数，预绑定设备参数和最大序列长度\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,        # 原始数据整理函数\n",
    "    device=device,            # 指定计算设备（CPU/GPU）\n",
    "    allowed_max_length=1024   # 允许的最大序列长度（防止内存溢出）\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a",
   "metadata": {
    "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a"
   },
   "source": [
    "- Next, we instantiate the data loaders similar to previous chapters, except that we now provide our own collate function for the batching process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "BtWkgir6Hlpe",
   "metadata": {
    "id": "BtWkgir6Hlpe"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 数据加载配置参数\n",
    "num_workers = 0    # 数据加载子进程数（0表示主进程加载）\n",
    "batch_size = 8      # 每个批次的样本数量\n",
    "\n",
    "# 设置随机种子保证可重复性\n",
    "torch.manual_seed(123)  # 固定所有随机数生成器的种子\n",
    "\n",
    "# 创建训练数据集和加载器\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)  # 自定义指令数据集\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,          # 使用预定义的批次大小\n",
    "    collate_fn=customized_collate_fn, # 使用预配置的数据整理函数\n",
    "    shuffle=True,                   # 每个epoch打乱数据顺序，确保模型不会记忆数据顺序\n",
    "    drop_last=True,                 # 丢弃最后不完整的批次，避免最后不完整批次影响梯度计算\n",
    "    num_workers=num_workers         # 保持加载进程数配置\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8a325d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38406c3",
   "metadata": {},
   "source": [
    "该输出表明，第一个输入批次和目标批次的维度为 8 × 61，其中 8 是批次大小，61 是该批次中每个训练样本的词元数量。第二个输入批次和目标批次中的词元数量则有 76个，与第一个不同。由于我们使用了自定义的聚合函数，因此数据加载器能够创建不同长度的批次。在 7.5 节中，我们将加载一个预训练的大语言模型，并利用这个数据加载器对该模型进行微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d097dc8-ad34-4f05-b435-e4147965f532",
   "metadata": {
    "id": "1d097dc8-ad34-4f05-b435-e4147965f532"
   },
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0",
   "metadata": {
    "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0"
   },
   "source": [
    "- Let's see what the dimensions of the resulting input and target batches look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "GGs1AI3vHpnX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGs1AI3vHpnX",
    "outputId": "f6a74c8b-1af3-4bc1-b48c-eda64b0200d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e8dd7-d46a-4cc3-8a7e-c1d31e1b4657",
   "metadata": {
    "id": "0c8e8dd7-d46a-4cc3-8a7e-c1d31e1b4657"
   },
   "source": [
    "- As we can see based on the output above, all batches have a batch size of 8 but a different length, as expected\n",
    "- Let's also double-check that the inputs contain the `<|endoftext|>` padding tokens corresponding to token ID 50256 by printing the contents of the first training example in the `inputs` batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
    "outputId": "1b8ad342-2b5b-4f12-ad1a-3cb2a6c712ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 42779,   597, 24993, 10135,   287,   262,\n",
      "         1813,  6827,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
      "          307,   315,  4135, 11376,   373,  5901,   351, 12734,    13,   198,\n",
      "          198, 21017, 18261,    25,   198,   464,  4950, 11376,   373,  5901,\n",
      "          351, 12734,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f3647-8971-4006-89e0-6a2a1ec1d360",
   "metadata": {
    "id": "5f1f3647-8971-4006-89e0-6a2a1ec1d360"
   },
   "source": [
    "- Similarly, we visually double-check that the targets contain the -100 placeholder tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
    "outputId": "5e8c23f8-6a05-4c13-9f92-373b75b57ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 42779,   597, 24993, 10135,   287,   262,  1813,\n",
      "         6827,    13,   198,   198, 21017, 23412,    25,   198,   464,   307,\n",
      "          315,  4135, 11376,   373,  5901,   351, 12734,    13,   198,   198,\n",
      "        21017, 18261,    25,   198,   464,  4950, 11376,   373,  5901,   351,\n",
      "        12734,    13, 50256,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aad445-8f19-4238-b9bf-db80767fb91a",
   "metadata": {
    "id": "d6aad445-8f19-4238-b9bf-db80767fb91a"
   },
   "source": [
    "## 7.5 Loading a pretrained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c07d1-4fc9-4846-94cf-b11a085a667b",
   "metadata": {
    "id": "5a5c07d1-4fc9-4846-94cf-b11a085a667b"
   },
   "source": [
    "- In this section, we load a pretrained GPT model using the same code that we used in section 5.5 of chapter 5 and section 6.4 in chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b438f-88af-413f-96a9-f059c6c55fc4",
   "metadata": {
    "id": "8d1b438f-88af-413f-96a9-f059c6c55fc4"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-4.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68eda7-e02e-4caa-846b-ca6dbd396ca2",
   "metadata": {
    "id": "8c68eda7-e02e-4caa-846b-ca6dbd396ca2"
   },
   "source": [
    "- However, instead of loading the smallest 124 million parameter model, we load the medium version with 355 million parameters since the 124 million model is too small for achieving qualitatively reasonable results via instruction finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
    "outputId": "386ebd49-51d7-4a62-c590-91cdccce5fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "# from llms_from_scratch.ch05 import download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5",
   "metadata": {
    "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5"
   },
   "source": [
    "- Before we start finetuning the model in the next section, let's see how it performs on one of the validation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088252d6",
   "metadata": {},
   "source": [
    "现在，让我们先花一些时间，通过将模型输出与预期的回复进行比较，来评估预训练的大语言模型在验证任务上的表现。这将为我们提供一个模型的基准性能指标，该指标反映了模型在未经微调的情况下在指令遵循任务中的表现情况，并能帮助我们更好地理解微调后的效果。下面我们将使用验证集中第一个样本进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
    "outputId": "c1276a91-e7da-495b-be0f-70a96872dbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa",
   "metadata": {
    "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch05 import (\n",
    "#    generate_text_simple,\n",
    "#    text_to_token_ids,\n",
    "#    token_ids_to_text\n",
    "# )\n",
    "\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e2fda5-f796-4954-8f72-1dd1123e3344",
   "metadata": {
    "id": "36e2fda5-f796-4954-8f72-1dd1123e3344"
   },
   "source": [
    "- Note that the `generate` function we used in previous chapters returns the combined input and output text, which was convenient in the previous section for creating legible text\n",
    "- To isolate the response, we can subtract the length of the instruction from the start of the `generated_text`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5784be",
   "metadata": {},
   "source": [
    "使用 generate 函数生成模型的回复，我们在第5章中使用过这个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
    "outputId": "3e231f03-c5dc-4397-8778-4995731176a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "response_text = (\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44080b2-a4c5-4520-a797-549519f66a3e",
   "metadata": {
    "id": "d44080b2-a4c5-4520-a797-549519f66a3e"
   },
   "source": [
    "- As we can see, the model is not capable of following the instructions, yet; it creates a \"Response\" section but it simply repeats the original input sentence as well as the instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d27b9d-a942-4cf5-b797-848c5f01e723",
   "metadata": {
    "id": "70d27b9d-a942-4cf5-b797-848c5f01e723"
   },
   "source": [
    "## 7.6 Finetuning the LLM on instruction data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b2a39-88b4-44d8-8c85-1c5b0cd6cc4a",
   "metadata": {
    "id": "314b2a39-88b4-44d8-8c85-1c5b0cd6cc4a"
   },
   "source": [
    "- In this section, we finetune the model\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-5.webp?1\" width=500px>\n",
    "\n",
    "- Note that we can reuse all the loss calculation and training functions that we used in previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65444865-df87-4d98-9faf-875e1c4be860",
   "metadata": {
    "id": "65444865-df87-4d98-9faf-875e1c4be860"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch05 import (\n",
    "#    calc_loss_loader,\n",
    "#    train_model_simple,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00",
   "metadata": {
    "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00"
   },
   "source": [
    "- Let's calculate the initial training and validation set loss before we start training (as in previous chapters, the goal is to minimize the loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
    "outputId": "a3f5e1b0-093a-4c51-e7fc-c9cac48c2ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8258963584899903\n",
      "Validation loss: 3.761921453475952\n"
     ]
    }
   ],
   "source": [
    "# 将模型迁移到指定设备（GPU/CPU）\n",
    "model.to(device)  \n",
    "\n",
    "# 固定随机种子保证结果可复现\n",
    "torch.manual_seed(123)  \n",
    "\n",
    "# 禁用梯度计算以加速推理过程\n",
    "with torch.no_grad():\n",
    "    # 计算训练集损失（仅用5个批次估算）\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    # 计算验证集损失（仅用5个批次估算）\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "# 输出训练/验证损失指标\n",
    "print(\"Training loss:\", train_loss)    # 显示训练损失\n",
    "print(\"Validation loss:\", val_loss)   # 显示验证损失\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6da8f-15b3-42b0-a136-619b7a35c3e9",
   "metadata": {
    "id": "12a6da8f-15b3-42b0-a136-619b7a35c3e9"
   },
   "source": [
    "- Note that the training is a bit more expensive than in previous chapters since we are using a larger model (355 million instead of 124 million parameters)\n",
    "- The runtimes for various devices are shown for reference below (running this notebook on a compatible GPU device requires no changes to the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b57fb-e689-4550-931c-6d34a932487c",
   "metadata": {
    "id": "db4b57fb-e689-4550-931c-6d34a932487c"
   },
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    \n",
    "| Model              | Device                | Runtime for 2 Epochs |\n",
    "|--------------------|-----------------------|----------------------|\n",
    "| gpt2-medium (355M) | CPU (M3 MacBook Air)  | 15.78 minutes        |\n",
    "| gpt2-medium (355M) | GPU (M3 MacBook Air)  | 10.77 minutes        |\n",
    "| gpt2-medium (355M) | GPU (L4)              | 1.83 minutes         |\n",
    "| gpt2-medium (355M) | GPU (A100)            | 0.86 minutes         |\n",
    "| gpt2-small (124M)  | CPU (M3 MacBook Air)  | 5.74 minutes         |\n",
    "| gpt2-small (124M)  | GPU (M3 MacBook Air)  | 3.73 minutes         |\n",
    "| gpt2-small (124M)  | GPU (L4)              | 0.69 minutes         |\n",
    "| gpt2-small (124M)  | GPU (A100)            | 0.39 minutes         |\n",
    "\n",
    "</div>\n",
    "\n",
    "- I ran this notebook using the `\"gpt2-medium (355M)\"` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
    "outputId": "ecb9a3dd-97c0-492d-8a51-fbd175bb139b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102\n",
      "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.945\n",
      "Ep 1 (Step 000015): Train loss 0.856, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.753, Val loss 0.859\n",
      "Ep 1 (Step 000030): Train loss 0.798, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
      "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.790\n",
      "Ep 1 (Step 000050): Train loss 0.662, Val loss 0.783\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.764\n",
      "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
      "Ep 1 (Step 000065): Train loss 0.652, Val loss 0.735\n",
      "Ep 1 (Step 000070): Train loss 0.532, Val loss 0.729\n",
      "Ep 1 (Step 000075): Train loss 0.569, Val loss 0.729\n",
      "Ep 1 (Step 000080): Train loss 0.605, Val loss 0.725\n",
      "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.709\n",
      "Ep 1 (Step 000090): Train loss 0.562, Val loss 0.691\n",
      "Ep 1 (Step 000095): Train loss 0.500, Val loss 0.681\n",
      "Ep 1 (Step 000100): Train loss 0.502, Val loss 0.677\n",
      "Ep 1 (Step 000105): Train loss 0.564, Val loss 0.670\n",
      "Ep 1 (Step 000110): Train loss 0.555, Val loss 0.667\n",
      "Ep 1 (Step 000115): Train loss 0.508, Val loss 0.664\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.672\n",
      "Ep 2 (Step 000125): Train loss 0.450, Val loss 0.687\n",
      "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.682\n",
      "Ep 2 (Step 000135): Train loss 0.405, Val loss 0.681\n",
      "Ep 2 (Step 000140): Train loss 0.409, Val loss 0.681\n",
      "Ep 2 (Step 000145): Train loss 0.368, Val loss 0.681\n",
      "Ep 2 (Step 000150): Train loss 0.381, Val loss 0.676\n",
      "Ep 2 (Step 000155): Train loss 0.412, Val loss 0.676\n",
      "Ep 2 (Step 000160): Train loss 0.415, Val loss 0.684\n",
      "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.686\n",
      "Ep 2 (Step 000170): Train loss 0.323, Val loss 0.683\n",
      "Ep 2 (Step 000175): Train loss 0.337, Val loss 0.671\n",
      "Ep 2 (Step 000180): Train loss 0.392, Val loss 0.658\n",
      "Ep 2 (Step 000185): Train loss 0.415, Val loss 0.660\n",
      "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.651\n",
      "Ep 2 (Step 000195): Train loss 0.330, Val loss 0.638\n",
      "Ep 2 (Step 000200): Train loss 0.310, Val loss 0.638\n",
      "Ep 2 (Step 000205): Train loss 0.351, Val loss 0.634\n",
      "Ep 2 (Step 000210): Train loss 0.366, Val loss 0.632\n",
      "Ep 2 (Step 000215): Train loss 0.396, Val loss 0.636\n",
      "Ep 2 (Step 000220): Train loss 0.300, Val loss 0.648\n",
      "Ep 2 (Step 000225): Train loss 0.348, Val loss 0.661\n",
      "Ep 2 (Step 000230): Train loss 0.294, Val loss 0.657\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Training completed in 39.08 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time  # 导入时间模块用于计算训练耗时\n",
    "\n",
    "# 记录训练开始时间戳\n",
    "start_time = time.time()  \n",
    "\n",
    "# 固定随机种子保证训练可复现\n",
    "torch.manual_seed(123)  \n",
    "\n",
    "# 初始化AdamW优化器（适合transformer模型）\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.00005,          # 较低学习率用于微调\n",
    "    weight_decay=0.1     # 较强的权重衰减防止过拟合\n",
    ")\n",
    "\n",
    "num_epochs = 2  # 设置训练周期数\n",
    "\n",
    "# 执行模型训练流程\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs,  # 总训练轮次\n",
    "    eval_freq=5,            # 每5个batch验证一次 \n",
    "    eval_iter=5,            # 每次验证用5个batch\n",
    "    start_context=format_input(val_data[0]),  # 取验证集首样本作为生成起点\n",
    "    tokenizer=tokenizer     # 传入分词器用于文本生成\n",
    ")\n",
    "\n",
    "# 计算总训练时长（分钟）\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")  # 保留两位小数输出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ise3wGjlB-iq",
   "metadata": {
    "id": "Ise3wGjlB-iq"
   },
   "source": [
    "- As we can see based on the outputs above, the model trains well, as we can tell based on the decreasing training loss and validation loss values\n",
    "- Furthermore, based on the response text printed after each epoch, we can see that the model correctly follows the instruction to convert the input sentence `'The chef cooks the meal every day.'` into passive voice `'The meal is cooked every day by the chef.'` (We will properly format and evaluate the responses in a later section)\n",
    "- Finally, let's take a look at the training and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
    "outputId": "2f5c99e0-7ed0-4f42-d67c-e07c375e6158"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_losses\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch05 import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777e0c4-d82c-46d8-84fb-1376c4f8bae0",
   "metadata": {
    "id": "6777e0c4-d82c-46d8-84fb-1376c4f8bae0"
   },
   "source": [
    "- As we can see, the loss decreases sharply at the beginning of the first epoch, which means the model starts learning quickly\n",
    "- We can see that slight overfitting sets in at around 1 training epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b79a47-13f9-4d1f-87b1-3339bafaf2a3",
   "metadata": {
    "id": "87b79a47-13f9-4d1f-87b1-3339bafaf2a3"
   },
   "source": [
    "## 7.7 Extracting and saving responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25cc88-1758-4dd0-b8bf-c044cbf2dd49",
   "metadata": {
    "id": "5a25cc88-1758-4dd0-b8bf-c044cbf2dd49"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-6.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17510e9d-7727-4d58-ba9a-d82ec23c1427",
   "metadata": {
    "id": "17510e9d-7727-4d58-ba9a-d82ec23c1427"
   },
   "source": [
    "- In this section, we save the test set responses for scoring in the next section\n",
    "- We also save a copy of the model for future use\n",
    "- But first, let's take a brief look at the responses generated by the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "VQ2NZMbfucAc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ2NZMbfucAc",
    "outputId": "066c56ff-b52a-4ee6-eae7-1bddfc74d0c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子保证结果可复现\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 遍历前3个测试样本\n",
    "for entry in test_data[:3]:\n",
    "    # 格式化输入文本（添加指令模板）\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    # 生成token序列（使用模型自回归生成）\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,      # 限制生成的最大token数量\n",
    "        context_size=BASE_CONFIG[\"context_length\"],  # 使用配置中的上下文长度\n",
    "        eos_id=50256             # 结束符token ID（对标GPT-2）\n",
    "    )\n",
    "    \n",
    "    # 将token序列解码为文本\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    \n",
    "    # 提取模型响应部分（去除输入文本和模板标记）\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]  # 截取输入文本之后的内容\n",
    "        .replace(\"### Response:\", \"\")     # 移除响应模板标记\n",
    "        .strip()                          # 去除首尾空白\n",
    "    )\n",
    "\n",
    "    # 打印对比结果\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")  # 标注正确答案\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")  # 显示模型生成结果\n",
    "    print(\"-------------------------------------\")  # 分隔线\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab64c1-586f-4939-8def-23feeb1b3599",
   "metadata": {
    "id": "49ab64c1-586f-4939-8def-23feeb1b3599"
   },
   "source": [
    "- As we can see based on the test set instructions, given responses, and the model's responses, the model performs relatively well\n",
    "- The answers to the first and last instructions are clearly correct\n",
    "- The second answer is close; the model answers with \"cumulus cloud\" instead of \"cumulonimbus\" (however, note that cumulus clouds can develop into cumulonimbus clouds, which are capable of producing thunderstorms)\n",
    "- Most importantly, we can see that model evaluation is not as straightforward as in the previous chapter, where we just had to calculate the percentage of correct spam/non-spam class labels to obtain the classification accuracy\n",
    "- In practice, instruction-finetuned LLMs such as chatbots are evaluated via multiple approaches\n",
    "  - short-answer and multiple choice benchmarks such as MMLU (\"Measuring Massive Multitask Language Understanding\", [https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)), which test the knowledge of a model\n",
    "  - human preference comparison to other LLMs, such as LMSYS chatbot arena ([https://arena.lmsys.org](https://arena.lmsys.org))\n",
    "  - automated conversational benchmarks, where another LLM like GPT-4 is used to evaluate the responses, such as AlpacaEval ([https://tatsu-lab.github.io/alpaca_eval/](https://tatsu-lab.github.io/alpaca_eval/))\n",
    "\n",
    "- In the next section, we will use an approach similar to AlpacaEval and use another LLM to evaluate the responses of our model; however, we will use our own test set instead of using a publicly available benchmark dataset\n",
    "- For this, we add the model response to the `test_data` dictionary and save it as a `\"instruction-data-with-response.json\"` file for record-keeping so that we can load and analyze it in separate Python sessions if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-PNGKzY4snKP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PNGKzY4snKP",
    "outputId": "37b22a62-9860-40b7-c46f-b297782b944c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 批量处理所有测试数据（带进度条显示）\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, entry \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[43mtest_data\u001b[49m), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_data)):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 格式化输入文本（添加指令模板）\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m format_input(entry)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# 生成模型响应token序列\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# 导入进度条库（用于显示处理进度）\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 批量处理所有测试数据（带进度条显示）\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    # 格式化输入文本（添加指令模板）\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    # 生成模型响应token序列\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),  # 文本转token并加载到设备\n",
    "        max_new_tokens=256,        # 最大生成token数限制\n",
    "        context_size=BASE_CONFIG[\"context_length\"],  # 使用预设上下文长度\n",
    "        eos_id=50256               # GPT-2的结束符token ID\n",
    "    )\n",
    "    \n",
    "    # 解码token序列为文本\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    \n",
    "    # 提取并清理模型响应（移除输入文本和模板标记）\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "    \n",
    "    # 记录模型响应到测试数据字典\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "# 将带模型响应的测试数据保存为JSON文件\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)  # 使用缩进格式化保存（便于人工阅读）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d6fa7-d162-44c3-bef1-4013c027b155",
   "metadata": {
    "id": "228d6fa7-d162-44c3-bef1-4013c027b155"
   },
   "source": [
    "- Let's double-check one of the entries to see whether the responses have been added to the `test_data` dictionary correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u-AvCCMTnPSE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-AvCCMTnPSE",
    "outputId": "7bcd9600-1446-4829-b773-5259b13d256a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a",
   "metadata": {
    "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a"
   },
   "source": [
    "- Finally, we also save the model in case we want to reuse it in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cBU0iHmVfOI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cBU0iHmVfOI",
    "outputId": "135849ed-9acd-43a2-f438-053d07dae9b2",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CHOOSE_MODEL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mre\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[ ()]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[43mCHOOSE_MODEL\u001b[49m)\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-sft.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), file_name)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CHOOSE_MODEL' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")\n",
    "\n",
    "# Load model via\n",
    "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obgoGI89dgPm",
   "metadata": {
    "id": "obgoGI89dgPm"
   },
   "source": [
    "## 7.8 Evaluating the finetuned LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b9d30-7336-499f-abb5-4a21be3129f5",
   "metadata": {
    "id": "805b9d30-7336-499f-abb5-4a21be3129f5"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-7.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2b9d3-b6ff-4533-a89d-7b66079b4fd1",
   "metadata": {
    "id": "68d2b9d3-b6ff-4533-a89d-7b66079b4fd1"
   },
   "source": [
    "- In this section, we automate the response evaluation of the finetuned LLM using another, larger LLM\n",
    "- In particular, we use an instruction-finetuned 8-billion-parameter Llama 3 model by Meta AI that can be run locally via ollama ([https://ollama.com](https://ollama.com))\n",
    "- (Alternatively, if you prefer using a more capable LLM like GPT-4 via the OpenAI API, please see the [llm-instruction-eval-openai.ipynb](../03_model-evaluation/llm-instruction-eval-openai.ipynb) notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea427a30-36ba-44e3-bb1f-eb0d7008d6e9",
   "metadata": {
    "id": "ea427a30-36ba-44e3-bb1f-eb0d7008d6e9"
   },
   "source": [
    "- Ollama is an application to run LLMs efficiently\n",
    "- It is a wrapper around llama.cpp ([https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)), which implements LLMs in pure C/C++ to maximize efficiency\n",
    "- Note that it is a tool for using LLMs to generate text (inference), not training or finetuning LLMs\n",
    "- Before running the code below, install ollama by visiting [https://ollama.com](https://ollama.com) and following the instructions (for instance, clicking on the \"Download\" button and downloading the ollama application for your operating system)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a2fc7-282d-47ec-a987-ed0a23ed6822",
   "metadata": {
    "id": "747a2fc7-282d-47ec-a987-ed0a23ed6822"
   },
   "source": [
    "- For macOS and Windows users, click on the ollama application you downloaded; if it prompts you to install the command line usage, say \"yes\"\n",
    "- Linux users can use the installation command provided on the ollama website\n",
    "\n",
    "- In general, before we can use ollama from the command line, we have to either start the ollama application or run `ollama serve` in a separate terminal\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/ollama-run.webp?1\" width=700px>\n",
    "\n",
    "\n",
    "- With the ollama application or `ollama serve` running in a different terminal, on the command line, execute the following command to try out the 8-billion-parameter Llama 3 model (the model, which takes up 4.7 GB of storage space, will be automatically downloaded the first time you execute this command)\n",
    "\n",
    "```bash\n",
    "# 8B model\n",
    "ollama run llama3\n",
    "```\n",
    "\n",
    "\n",
    "The output looks like as follows\n",
    "\n",
    "```\n",
    "$ ollama run llama3\n",
    "pulling manifest\n",
    "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB\n",
    "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB\n",
    "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B\n",
    "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B\n",
    "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B\n",
    "verifying sha256 digest\n",
    "writing manifest\n",
    "removing any unused layers\n",
    "success\n",
    "```\n",
    "\n",
    "- Note that `llama3` refers to the instruction finetuned 8-billion-parameter Llama 3 model\n",
    "\n",
    "- Using ollama with the `\"llama3\"` model (a 8B parameter model) requires 16 GB of RAM; if this is not supported by your machine, you can try the smaller model, such as the 3.8B parameter phi-3 model by setting `model = \"phi-3\"`, which only requires 8 GB of RAM\n",
    "\n",
    "- Alternatively, you can also use the larger 70-billion-parameter Llama 3 model, if your machine supports it, by replacing `llama3` with `llama3:70b`\n",
    "\n",
    "- After the download has been completed, you will see a command line prompt that allows you to chat with the model\n",
    "\n",
    "- Try a prompt like \"What do llamas eat?\", which should return an output similar to the following\n",
    "\n",
    "```\n",
    ">>> What do llamas eat?\n",
    "Llamas are ruminant animals, which means they have a four-chambered\n",
    "stomach and eat plants that are high in fiber. In the wild, llamas\n",
    "typically feed on:\n",
    "1. Grasses: They love to graze on various types of grasses, including tall\n",
    "grasses, wheat, oats, and barley.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b341c-ba0e-40bb-a52c-cb328bbd1fe4",
   "metadata": {
    "id": "7b7b341c-ba0e-40bb-a52c-cb328bbd1fe4"
   },
   "source": [
    "- You can end this session using the input `/bye`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf3e02-8ca0-4edf-be23-60625a5b14e3",
   "metadata": {
    "id": "faaf3e02-8ca0-4edf-be23-60625a5b14e3"
   },
   "source": [
    "- The following code checks whether the ollama session is running correctly before proceeding to use ollama to evaluate the test set responses we generated in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e8570-071e-48a2-aa38-64d7be35f288",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "026e8570-071e-48a2-aa38-64d7be35f288",
    "outputId": "e30d3533-e1f5-4aa9-b24f-33273fc7b30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c9b00-e3cd-4092-83c3-6e48b5cf65b0",
   "metadata": {
    "id": "723c9b00-e3cd-4092-83c3-6e48b5cf65b0"
   },
   "outputs": [],
   "source": [
    "# This cell is optional; it allows you to restart the notebook\n",
    "# and only run section 7.7 without rerunning any of the previous code\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = \"instruction-data-with-response.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3464705-d026-4594-977f-fb357e51c3a9",
   "metadata": {
    "id": "b3464705-d026-4594-977f-fb357e51c3a9"
   },
   "source": [
    "- Now, an alternative way to the `ollama run` command we used earlier to interact with the model is via its REST API in Python via the following function\n",
    "- Before you run the next cells in this notebook, make sure that ollama is still running (the previous code cells should print `\"Ollama running: True\"`)\n",
    "- Next, run the following code cell to query the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae0e10-2b28-42ce-8ea2-d9366a58088f",
   "metadata": {
    "id": "e3ae0e10-2b28-42ce-8ea2-d9366a58088f",
    "outputId": "cc43acb3-8216-43cf-c77d-71d4089dc96c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "    prompt,\n",
    "    model=\"llama3\",\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {     # Settings below are required for deterministic responses\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ae28f-0f8c-4fda-aeef-e7e3046249cc",
   "metadata": {
    "id": "207ae28f-0f8c-4fda-aeef-e7e3046249cc"
   },
   "source": [
    "- Now, using the `query_model` function we defined above, we can evaluate the responses of our finetuned model; let's try it out on the first 3 test set responses we looked at in a previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b839d4-064d-4178-b2d7-01691b452e5e",
   "metadata": {
    "id": "86b839d4-064d-4178-b2d7-01691b452e5e",
    "outputId": "1c755ee1-bded-4450-9b84-1466724f389a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "\n",
      "Score:\n",
      ">> I'd rate the model response \"The car is as fast as a bullet.\" an 85 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The response uses a simile correctly, comparing the speed of the car to something else (in this case, a bullet).\n",
      "* The comparison is relevant and makes sense, as bullets are known for their high velocity.\n",
      "* The phrase \"as fast as\" is used correctly to introduce the simile.\n",
      "\n",
      "The only reason I wouldn't give it a perfect score is that some people might find the comparison slightly less vivid or evocative than others. For example, comparing something to lightning (as in the original response) can be more dramatic and attention-grabbing. However, \"as fast as a bullet\" is still a strong and effective simile that effectively conveys the idea of the car's speed.\n",
      "\n",
      "Overall, I think the model did a great job!\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "\n",
      "Score:\n",
      ">> I'd score this model response as 40 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The model correctly identifies that thunderstorms are related to clouds (correctly identifying the type of phenomenon).\n",
      "* However, it incorrectly specifies the type of cloud associated with thunderstorms. Cumulus clouds are not typically associated with thunderstorms; cumulonimbus clouds are.\n",
      "* The response lacks precision and accuracy in its description.\n",
      "\n",
      "Overall, while the model attempts to address the instruction, it provides an incorrect answer, which is a significant error.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> I'd rate my own response as 95 out of 100. Here's why:\n",
      "\n",
      "* The response accurately answers the question by naming the author of 'Pride and Prejudice' as Jane Austen.\n",
      "* The response is concise and clear, making it easy to understand.\n",
      "* There are no grammatical errors or ambiguities that could lead to confusion.\n",
      "\n",
      "The only reason I wouldn't give myself a perfect score is that the response is slightly redundant - it's not necessary to rephrase the question in the answer. A more concise response would be simply \"Jane Austen.\"\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fec453-631f-4ff5-a922-44c3c451942d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note: Better evaluation prompt**\n",
    "\n",
    "- [A reader (Ayoosh Kathuria) suggested](https://github.com/rasbt/LLMs-from-scratch/discussions/449) a longer, improved prompt that evaluates responses on a scale of 1–5 (instead of 1 to 100) and employs a grading rubric, resulting in more accurate and less noisy evaluations:\n",
    "\n",
    "```\n",
    "prompt = \"\"\"\n",
    "You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.\n",
    "You will be given an instruction, a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing the evaluation criteria.\n",
    "Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "Here is the rubric you should use to build your answer:\n",
    "1: The response fails to address the instructions, providing irrelevant, incorrect, or excessively verbose information that detracts from the user's request.\n",
    "2: The response partially addresses the instructions but includes significant inaccuracies, irrelevant details, or excessive elaboration that detracts from the main task.\n",
    "3: The response follows the instructions with some minor inaccuracies or omissions. It is generally relevant and clear, but may include some unnecessary details or could be more concise.\n",
    "4: The response adheres to the instructions, offering clear, accurate, and relevant information in a concise manner, with only occasional, minor instances of excessive detail or slight lack of clarity.\n",
    "5: The response fully adheres to the instructions, providing a clear, accurate, and relevant answer in a concise and efficient manner. It addresses all aspects of the request without unnecessary details or elaboration\n",
    "\n",
    "Provide your feedback as follows:\n",
    "\n",
    "Feedback:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the instruction, the reference answer, and the response.\n",
    "\n",
    "Instruction: {instruction}\n",
    "Reference Answer: {reference}\n",
    "Answer: {answer}\n",
    "\n",
    "\n",
    "Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.\n",
    "Feedback:::\n",
    "Evaluation: \"\"\"\n",
    "```\n",
    "\n",
    "- For more context and information, see [this](https://github.com/rasbt/LLMs-from-scratch/discussions/449) GitHub discussion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114fd65-9cfb-45f6-ab74-8331da136bf3",
   "metadata": {
    "id": "b114fd65-9cfb-45f6-ab74-8331da136bf3"
   },
   "source": [
    "- As we can see, the Llama 3 model provides a reasonable evaluation and also gives partial points if a model is not entirely correct, as we can see based on the \"cumulus cloud\" answer\n",
    "- Note that the previous prompt returns very verbose evaluations; we can tweak the prompt to generate integer responses in the range between 0 and 100 (where 100 is best) to calculate an average score for our model\n",
    "- The evaluation of the 110 entries in the test set takes about 1 minute on an M3 MacBook Air laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7bca69-97c4-47a5-9aa0-32f116fa37eb",
   "metadata": {
    "id": "9d7bca69-97c4-47a5-9aa0-32f116fa37eb",
    "outputId": "110223c0-90ca-481d-b2d2-f6ac46d3c4f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|████████████████████████| 110/110 [01:10<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 50.32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f08d5-9ada-4301-9ebc-f0533c76d3f2",
   "metadata": {
    "id": "407f08d5-9ada-4301-9ebc-f0533c76d3f2"
   },
   "source": [
    "- Our model achieves an average score of above 50, which we can use as a reference point to compare the model to other models or to try out other training settings that may improve the model\n",
    "- Note that ollama is not fully deterministic across operating systems (as of this writing), so the numbers you are getting might slightly differ from the ones shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6408768b-2784-44f1-b48e-aed0c1eb9b94",
   "metadata": {
    "id": "6408768b-2784-44f1-b48e-aed0c1eb9b94"
   },
   "source": [
    "- For reference, the original\n",
    "  - Llama 3 8B base model achieves a score of 58.51\n",
    "  - Llama 3 8B instruct model achieves a score of 82.65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d7325-284a-446c-92a1-5aa8acc52dee",
   "metadata": {
    "id": "412d7325-284a-446c-92a1-5aa8acc52dee"
   },
   "source": [
    "## 7.9 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tIbNMluCDjVM",
   "metadata": {
    "id": "tIbNMluCDjVM"
   },
   "source": [
    "### 7.9.1 What's next\n",
    "\n",
    "- This marks the final chapter of this book\n",
    "- We covered the major steps of the LLM development cycle: implementing an LLM architecture, pretraining an LLM, and finetuning it\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/final-overview.webp?1\" width=500px>\n",
    "\n",
    "- An optional step that is sometimes followed after instruction finetuning, as described in this chapter, is preference finetuning\n",
    "- Preference finetuning process can be particularly useful for customizing a model to better align with specific user preferences; see the [../04_preference-tuning-with-dpo](../04_preference-tuning-with-dpo) folder if you are interested in this\n",
    "\n",
    "- This GitHub repository also contains a large selection of additional bonus material you may enjoy; for more information, please see the [Bonus Material](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) section on this repository's README page\n",
    "\n",
    "### 7.9.2 Staying up to date in a fast-moving field\n",
    "\n",
    "- No code in this section\n",
    "\n",
    "### 7.9.3 Final words\n",
    "\n",
    "- I hope you enjoyed this journey of implementing an LLM from the ground up and coding the pretraining and finetuning functions\n",
    "- In my opinion, implementing an LLM from scratch is the best way to understand how LLMs work; I hope you gained a better understanding through this approach\n",
    "- While this book serves educational purposes, you may be interested in using different and more powerful LLMs for real-world applications\n",
    "  - For this, you may consider popular tools such as axolotl ([https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)) or LitGPT ([https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt)), which I help developing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9853e7f-a81a-4806-9728-be1690807185",
   "metadata": {
    "id": "f9853e7f-a81a-4806-9728-be1690807185"
   },
   "source": [
    "## Summary and takeaways\n",
    "\n",
    "- See the [./gpt_instruction_finetuning.py](./gpt_instruction_finetuning.py) script, a self-contained script for instruction finetuning\n",
    "- [./ollama_evaluate.py](./ollama_evaluate.py) is a standalone script based on section 7.8 that evaluates a JSON file containing \"output\" and \"response\" keys via Ollama and Llama 3\n",
    "- The [./load-finetuned-model.ipynb](./load-finetuned-model.ipynb) notebook illustrates how to load the finetuned model in a new session\n",
    "- You can find the exercise solutions in [./exercise-solutions.ipynb](./exercise-solutions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc51ec-e06c-4470-b626-48401a037851",
   "metadata": {
    "id": "b9cc51ec-e06c-4470-b626-48401a037851"
   },
   "source": [
    "## What's next?\n",
    "\n",
    "- Congrats on completing the book; in case you are looking for additional resources, I added several bonus sections to this GitHub repository that you might find interesting\n",
    "- The complete list of bonus materials can be viewed in the main README's [Bonus Material](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) section\n",
    "- To highlight a few of my favorites:\n",
    "  1. [Direct Preference Optimization (DPO) for LLM Alignment (From Scratch)](../04_preference-tuning-with-dpo/dpo-from-scratch.ipynb) implements a popular preference tuning mechanism to align the model from this chapter more closely with human preferences\n",
    "  2. [Llama 3.2 From Scratch (A Standalone Notebook)](../../ch05/07_gpt_to_llama/standalone-llama32.ipynb), a from-scratch implementation of Meta AI's popular Llama 3.2, including loading the official pretrained weights; if you are up to some additional experiments, you can replace the `GPTModel` model in each of the chapters with the `Llama3Model` class (it should work as a 1:1 replacement)\n",
    "  3. [Converting GPT to Llama](../../ch05/07_gpt_to_llama) contains code with step-by-step guides that explain the differences between GPT-2 and the various Llama models\n",
    "  4. [Understanding the Difference Between Embedding Layers and Linear Layers](../../ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb) is a conceptual explanation illustrating that the `Embedding` layer in PyTorch, which we use at the input stage of an LLM, is mathematically equivalent to a linear layer applied to one-hot encoded data\n",
    "- Happy further reading!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
